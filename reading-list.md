---
layout: page
title: Reading List
---

I use this page to keep track of the papers I've read, especially those I find particularly cool, as well as papers which I intend to read in the future.

## Reading Group Papers

These are the papers I read during the 2017-2018 school year as part of the Hutch Research reading group.

#### Fall 2017

- [Recurrent Additive Networks](https://arxiv.org/pdf/1705.07393v2.pdf)

- [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/pdf/1608.03983.pdf)

- [Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks](https://arxiv.org/pdf/1709.03423v1.pdf)

- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

- [Learning Hierarchical Information Flow with Recurrent Neural Modules](https://arxiv.org/pdf/1706.05744.pdf)

- [Twin Networks: Using the Future as a Regularizer](https://arxiv.org/pdf/1708.06742.pdf)

- [Quasi-Recurrent Neural Networks](https://arxiv.org/pdf/1611.01576.pdf)

- [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf)

#### Winter 2018

 - [Understanding Intermediate Layers using Linear Classifier Probes](https://arxiv.org/pdf/1610.01644.pdf)
 
 - [Population Based Training of Neural Networks](https://arxiv.org/pdf/1711.09846.pdf)
 
 - [Black-Box Attacks Against RNN Based Malware Detection Algorithms](https://arxiv.org/pdf/1705.08131.pdf)
 
 - [Inductive Representation Learning on Large Graphs](https://arxiv.org/pdf/1706.02216.pdf)
 
 - [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://arxiv.org/pdf/1705.08292.pdf)

#### Spring 2018 (GAN Series)

 - [Generative Additive Networks](https://arxiv.org/pdf/1406.2661.pdf)
 
 - [Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498.pdf)
 
 - [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)
 
 - [Fader Networks: Manipulating Images by Sliding Attributes](https://arxiv.org/pdf/1706.00409.pdf)
 
 - [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/pdf/1710.10196.pdf)
 
 - [MaskGAN: Better Text Generation via Filling in the _____](https://arxiv.org/pdf/1801.07736.pdf)
 
 - [Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models](https://arxiv.org/pdf/1805.06605.pdf)

## NLP

#### General NLP

- [Universal Language Model Fine-tuning for Text Classification (Howard and Ruder, 2018)](https://arxiv.org/pdf/1801.06146.pdf) 

#### Automatic Sarcasm Detection

- [ICWSM - A Great Catchy Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Produce Reviews (Tsur et al., 2010)](http://kanagawa.lti.cs.cmu.edu/11719/sites/default/files/Tsur-Sarcasm.pdf)

- [Identifying Sarcasm in Twitter: A Closer Look (Gonzalez-Ibanez et al., 2011)](http://www.aclweb.org/anthology/P11-2102?CFID=24023237&CFTOKEN=4b3ce3611271a772-EB005048-FE47-3BDC-31F637B5AA62C4D3)

- [Sarcasm as Contrast Between a Positive Sentiment and Negative Situation (Riloff et al., 2013)](https://www.cs.utah.edu/~riloff/pdfs/official-emnlp13-sarcasm.pdf)

- [Harnessing Context Incongruity for Sarcasm Detection (Joshi et al., 2015)](http://www.aclweb.org/anthology/P15-2124)

- [Sarcasm Detection on Twitter: A Behavioral Modeling Approach (Rajadesingan et al., 2015)](https://dl.acm.org/citation.cfm?id=2685316)

- [Contextualized Sarcasm Detection on Twitter (Bamman and Smith, 2015)](https://homes.cs.washington.edu/~nasmith/papers/bamman+smith.icwsm15.pdf)

- [Automatic Sarcasm Detection: A Survey (Joshi et al., 2016)](https://arxiv.org/pdf/1602.03426.pdf)

- [Modelling Context with User Embeddings for Sarcasm Detection in Social Media (Amir et al., 2016)](https://arxiv.org/pdf/1607.00976.pdf)

- [Fracking Sarcasm using Neural Network (Ghosh and Veale, 2016)](http://www.aclweb.org/anthology/W16-0425)

- [A Deeper Look into Sarcastic Tweets using Deep Convolutional Neural Networks (Poria et al., 2017)](https://arxiv.org/pdf/1610.08815.pdf)

- [Tweet Sarcasm Detection Using Deep Neural Network (Zhang et al., 2017)](http://www.aclweb.org/anthology/C16-1231)

- [Expect the Unexpected: Harnessing Sentence Completion for Sarcasm Detection (Joshi et al., 2017)](https://arxiv.org/pdf/1707.06151.pdf)

#### Attention for NLP

- [A Convolutional Attention Model for Text Classification (Du et al., 2017)](http://tcci.ccf.org.cn/conference/2017/papers/1057.pdf)

- [ABCNN: Attention-Based CNN for Modeling Sentence Pairs (Yin et al., 2016)](http://www.aclweb.org/anthology/Q16-1019)

- [A Structured Self-Attentive Sentence Embedding (Lin et al., 2017)](https://arxiv.org/pdf/1703.03130.pdf)

## Miscellaneous

## TODO: Read These

- [Hierarchical Neural Story Generation (Fan et al., 2018)](https://arxiv.org/pdf/1805.04833.pdf)

- [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2016)](https://arxiv.org/pdf/1508.07909.pdf)
